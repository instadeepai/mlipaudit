# Copyright 2025 InstaDeep Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import zipfile
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any

from ase import Atom
from huggingface_hub import hf_hub_download
from mlip.models import ForceField
from pydantic import BaseModel, Field


class BenchmarkResult(BaseModel):
    """A base model for all benchmark results.

    Attributes:
        score: The final score for the benchmark between
            0 and 1.
    """

    score: float = Field(ge=0, le=1)


class ModelOutput(BaseModel):
    """A base model for all intermediate model outputs."""


class Benchmark(ABC):
    """An Abstract Base Class for structuring MLIP benchmark calculations.

    This class uses the Template Method pattern. Each concrete benchmark
    must implement the `run_model` and `analyze` methods. Benchmarks must
    be designed to first call `run_model` followed by `analyze`. Intermediate
    calculations generated by `run_model` will be stored in the instance variable
    `model_output`. Results generated by `analyze` will be stored in the instance
    variable `model_output`.

    Subclasses should also define the class attribute `name`, giving the
    benchmark a unique name, as well as `input_data_url` if necessary, specifying where
    any input data should be downloaded from.

    Attributes:
        name: The unique benchmark name that should be used to run the benchmark
            from the CLI and that will determine the output folder name for the result
            file.
        result_class: A reference to the type of `BenchmarkResult` that will determine
            the return type of ``self.analyze()``.
        atomic_species: The set of atomic species that are present in the benchmark's
            input files.
        skip_if_missing_species: Whether the benchmark should be skipped entirely if
            there are some atomic species that the model cannot handle. If False,
            the benchmark must have its own custom logic to handle missing atomic
            species. Defaults to True.
    """

    name: str = ""
    result_class: type[BenchmarkResult] | None = None

    atomic_species: set[str]
    skip_if_missing_species: bool = True

    def __init__(
        self,
        force_field: ForceField,
        data_input_dir: str | os.PathLike = "./data",
        fast_dev_run: bool = False,
    ) -> None:
        """Initializes the benchmark.

        Args:
            force_field: The force field model to be benchmarked.
            data_input_dir: The local input data directory. Defaults to
                "./data". If the subdirectory "{data_input_dir}/{benchmark_name}"
                exists, the benchmark expects the relevant data to be in there,
                otherwise it will download it from HuggingFace.
            fast_dev_run: Whether to do a fast developer run. Subclasses
                should ensure that when `True`, their benchmark runs in a
                much shorter timeframe, by running on a reduced number of
                test cases, for instance.
        """
        self.force_field = force_field
        self._handle_missing_atomic_species()
        self.fast_dev_run = fast_dev_run
        self.data_input_dir = Path(data_input_dir)

        self.model_output: ModelOutput | None = None

        self._download_data()

    def __init_subclass__(cls, **kwargs: Any):
        """Called when a class inherits from `Benchmark`.

        Used to validate that the required class attributes are defined.
        """
        super().__init_subclass__(**kwargs)
        if not cls.name:
            raise NotImplementedError(
                f"{cls.__name__} must override the 'name' attribute."
            )
        if cls.result_class is None:
            raise NotImplementedError(
                f"{cls.__name__} must override the 'result_class' attribute."
            )
        if cls.atomic_species is None:
            raise NotImplementedError(
                f"{cls.__name__} must override the 'atomic_species' attribute."
            )

    @classmethod
    def get_missing_atomic_species(cls, force_field: ForceField) -> set[str]:
        """Fetch the set of missing allowed atomic species from
        the force field to run the benchmark.

        Args:
            force_field: The force field model to be benchmarked.

        Returns:
            The set of atomic species that the model cannot handle
                that are part of the input data.
        """
        ff_allowed_atomic_species = set(
            Atom(symbol=atomic_number).symbol
            for atomic_number in force_field.allowed_atomic_numbers
        )
        missing_atomic_species = cls.atomic_species - ff_allowed_atomic_species
        return missing_atomic_species

    def _handle_missing_atomic_species(self):
        if self.skip_if_missing_species:
            missing_atomic_species = self.get_missing_atomic_species(self.force_field)
            if missing_atomic_species:
                raise ValueError(
                    f"The following atomic species are missing:"
                    f" {missing_atomic_species}"
                )

    @classmethod
    def check_can_run_model(cls, force_field: ForceField) -> bool:
        """Checks if a model can be run on a specific benchmark
        by ensuring that the model can handle all the atomic
        species in the benchmark's input files.

        Args:
            force_field: The force field model to be benchmarked.

        Returns:
            Whether the model can be run on the benchmark.
        """
        if cls.skip_if_missing_species:
            missing_atomic_species = cls.get_missing_atomic_species(force_field)
            if missing_atomic_species:
                return False

        return True

    def _download_data(self) -> None:
        """Download the data from the data input directory if not already exists."""
        already_exists = (self.data_input_dir / self.name).exists()
        if not already_exists:
            hf_hub_download(
                repo_id="InstaDeepAI/MLIPAudit-data",
                filename=f"{self.name}.zip",
                local_dir=self.data_input_dir,
                repo_type="dataset",
            )
            with zipfile.ZipFile(self.data_input_dir / f"{self.name}.zip", "r") as z:
                z.extractall(self.data_input_dir)

    @abstractmethod
    def run_model(self) -> None:
        """Generates any necessary data with `self.force_field`.

        Subclasses must implement this method. Raw data from simulations,
        single-point energy calculations or other types of calculations
        will be stored in the instance variable `model_output`.
        """
        pass

    @abstractmethod
    def analyze(self) -> BenchmarkResult:
        """Performs all post-inference or simulation analysis.

        Subclasses must implement this method. This method
        processes the raw data generated from the generation step
        to compute final metrics. Subclasses are also responsible
        for computing the final score for the benchmark.

        Returns:
            A class-specific instance of `BenchmarkResult`.
        """
        pass
